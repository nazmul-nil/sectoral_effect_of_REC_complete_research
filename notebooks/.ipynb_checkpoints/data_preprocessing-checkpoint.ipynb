{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a159537c-0aee-47ff-86fa-e9692ee7a180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Enhanced Preprocessing for Panel Data Econometrics\n",
      "============================================================\n",
      "\n",
      "üìä Step 1: Loading and exploring raw data...\n",
      "Initial dataset shape: (24211, 39)\n",
      "Number of series: 91\n",
      "Number of countries: 268\n",
      "\n",
      "üßπ Step 2: Initial cleaning and type conversion...\n",
      "Year columns identified: 35 columns from 1989 [YR1989] to 2023 [YR2023]\n",
      "‚úÖ Column names cleaned and data types converted\n",
      "\n",
      "üîÑ Step 3: Enhanced missing value treatment...\n",
      "Original missing values: 246,464\n",
      "Applying within-group temporal imputation...\n",
      "Missing after temporal imputation: 246,464\n",
      "‚úÖ Using only within-group temporal imputation (econometric best practice)\n",
      "\n",
      "üéØ Step 4: Enhanced outlier detection and treatment...\n",
      "Detecting outliers in growth indicators...\n",
      "Total outliers detected and winsorized: 5628\n",
      "‚úÖ Outlier treatment completed using statistical methods\n",
      "\n",
      "‚úÖ Step 5: Data quality validation...\n",
      "Data validation issues found and corrected: 0\n",
      "\n",
      "üåç Step 6: Enhanced country classification...\n",
      "‚úÖ Successfully loaded World Bank income classifications\n",
      "Income level distribution before filtering:\n",
      "Income Level\n",
      "High income            7007\n",
      "Upper middle income    4823\n",
      "Lower middle income    4641\n",
      "Low income             2457\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üîç Step 7: Enhanced sample filtering...\n",
      "Final income level distribution:\n",
      "Income Level\n",
      "Lower middle income    4914\n",
      "Upper middle income    4641\n",
      "Low income             2548\n",
      "Name: count, dtype: int64\n",
      "Final country count: 136\n",
      "\n",
      "üìä Step 8: Enhanced data availability assessment...\n",
      "Low income: 28 countries, 91 series, 66.0% data completion\n",
      "Upper middle income: 51 countries, 91 series, 71.5% data completion\n",
      "Lower middle income: 54 countries, 91 series, 74.3% data completion\n",
      "\n",
      "Applying enhanced missing data thresholds...\n",
      "Dropped 1,844 observations with insufficient data\n",
      "Retained 10,355 observations (84.9%)\n",
      "\n",
      "‚úÖ Step 9: Final validation and quality checks...\n",
      "‚úÖ No duplicate rows found\n",
      "‚úÖ All income classifications are valid\n",
      "\n",
      "============================================================\n",
      "üìã FINAL DATA QUALITY SUMMARY\n",
      "============================================================\n",
      "Final dataset shape: (10355, 40)\n",
      "Countries: 133\n",
      "Series indicators: 91\n",
      "Time span: 35 years\n",
      "Overall data completion rate: 83.3%\n",
      "\n",
      "üíæ Step 10: Saving enhanced dataset...\n",
      "‚úÖ Enhanced dataset saved: ../datasets/enhanced_growth_rates_emissions_energy_prod_income_level_country_df.csv\n",
      "‚úÖ Metadata saved: ../datasets/preprocessing_metadata.csv\n",
      "\n",
      "üéâ ENHANCED PREPROCESSING COMPLETED SUCCESSFULLY!\n",
      "============================================================\n",
      "Key improvements over original preprocessing:\n",
      "‚Ä¢ Removed problematic cross-series imputation\n",
      "‚Ä¢ Added statistical outlier detection and winsorization\n",
      "‚Ä¢ Enhanced data quality validation\n",
      "‚Ä¢ Improved missing data thresholds\n",
      "‚Ä¢ Added comprehensive documentation and metadata\n",
      "‚Ä¢ Follows econometric best practices for panel data\n",
      "Report saved: ../datasets/preprocessing_report.md\n",
      "\n",
      "üöÄ Dataset ready for GMM analysis!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üîß Enhanced Preprocessing for Panel Data Econometrics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: DATA LOADING AND INITIAL EXPLORATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìä Step 1: Loading and exploring raw data...\")\n",
    "\n",
    "df = pd.read_csv('../datasets/P_Data_Extract_From_World_Development_Indicators/0bcc63a9-526c-4673-98ef-4f8ebcf0fe7c_Data.csv')\n",
    "\n",
    "print(f\"Initial dataset shape: {df.shape}\")\n",
    "print(f\"Number of series: {df['Series Name'].nunique()}\")\n",
    "print(f\"Number of countries: {df['Country Name'].nunique()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: INITIAL CLEANING AND TYPE CONVERSION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüßπ Step 2: Initial cleaning and type conversion...\")\n",
    "\n",
    "# Convert World Bank missing value indicators to pandas NA\n",
    "df.replace([\"..\", \"...\", \"\"], pd.NA, inplace=True)\n",
    "\n",
    "# Identify year columns\n",
    "year_columns = [col for col in df.columns if 'YR' in col and '[' in col]\n",
    "non_year_columns = [col for col in df.columns if col not in year_columns]\n",
    "\n",
    "print(f\"Year columns identified: {len(year_columns)} columns from {year_columns[0]} to {year_columns[-1]}\")\n",
    "\n",
    "# Convert year columns to numeric\n",
    "df[year_columns] = df[year_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Clean column names\n",
    "df.columns = [col.replace(\" [YR\", \"_\").replace(\"]\", \"\") if \"YR\" in col else col for col in df.columns]\n",
    "year_columns = [col.replace(\" [YR\", \"_\").replace(\"]\", \"\") for col in year_columns]\n",
    "\n",
    "print(\"‚úÖ Column names cleaned and data types converted\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: ENHANCED MISSING VALUE TREATMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüîÑ Step 3: Enhanced missing value treatment...\")\n",
    "\n",
    "# Store original missing count for comparison\n",
    "original_missing = df[year_columns].isnull().sum().sum()\n",
    "print(f\"Original missing values: {original_missing:,}\")\n",
    "\n",
    "# Method 1: Within-group temporal imputation (STANDARD PRACTICE)\n",
    "print(\"Applying within-group temporal imputation...\")\n",
    "df[year_columns] = df.groupby([\"Country Name\", \"Series Name\"])[year_columns].transform(\n",
    "    lambda x: x.bfill().ffill()\n",
    ")\n",
    "\n",
    "# Count missing after temporal imputation\n",
    "after_temporal = df[year_columns].isnull().sum().sum()\n",
    "print(f\"Missing after temporal imputation: {after_temporal:,}\")\n",
    "\n",
    "# Method 2: REMOVED - Cross-series imputation (problematic for econometrics)\n",
    "# This step from original code is removed as it can introduce spurious correlations\n",
    "\n",
    "print(\"‚úÖ Using only within-group temporal imputation (econometric best practice)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: ENHANCED OUTLIER DETECTION AND TREATMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüéØ Step 4: Enhanced outlier detection and treatment...\")\n",
    "\n",
    "def detect_outliers_iqr(series, factor=1.5):\n",
    "    \"\"\"Detect outliers using IQR method\"\"\"\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - factor * IQR\n",
    "    upper_bound = Q3 + factor * IQR\n",
    "    return (series < lower_bound) | (series > upper_bound)\n",
    "\n",
    "def detect_outliers_zscore(series, threshold=3):\n",
    "    \"\"\"Detect outliers using Z-score method\"\"\"\n",
    "    z_scores = np.abs(stats.zscore(series, nan_policy='omit'))\n",
    "    return z_scores > threshold\n",
    "\n",
    "# Apply systematic outlier detection for growth variables\n",
    "growth_indicators = [col for col in df['Series Name'].unique() \n",
    "                    if pd.notna(col) and ('growth' in str(col).lower() or 'Growth' in str(col))]\n",
    "outlier_summary = {}\n",
    "\n",
    "print(\"Detecting outliers in growth indicators...\")\n",
    "for indicator in growth_indicators:\n",
    "    indicator_data = df[df['Series Name'] == indicator]\n",
    "    \n",
    "    for year_col in year_columns:\n",
    "        if year_col in indicator_data.columns:\n",
    "            series = indicator_data[year_col].dropna()\n",
    "            \n",
    "            if len(series) > 10:  # Need sufficient data\n",
    "                # Use IQR method for growth rates (more conservative)\n",
    "                outliers_iqr = detect_outliers_iqr(series, factor=2.0)  # More conservative\n",
    "                outlier_count = outliers_iqr.sum()\n",
    "                \n",
    "                if outlier_count > 0:\n",
    "                    outlier_summary[f\"{indicator}_{year_col}\"] = outlier_count\n",
    "                    \n",
    "                    # Cap extreme outliers instead of removing them\n",
    "                    upper_cap = series.quantile(0.95)\n",
    "                    lower_cap = series.quantile(0.05)\n",
    "                    \n",
    "                    # Apply winsorization using safer indexing\n",
    "                    mask = df['Series Name'] == indicator\n",
    "                    mask_indices = df.index[mask]\n",
    "                    \n",
    "                    # Apply clipping to the specific indices\n",
    "                    df.loc[mask_indices, year_col] = df.loc[mask_indices, year_col].clip(lower=lower_cap, upper=upper_cap)\n",
    "\n",
    "total_outliers = sum(outlier_summary.values())\n",
    "print(f\"Total outliers detected and winsorized: {total_outliers}\")\n",
    "print(\"‚úÖ Outlier treatment completed using statistical methods\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: DATA QUALITY VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n‚úÖ Step 5: Data quality validation...\")\n",
    "\n",
    "# Check for impossible values in specific indicators\n",
    "validation_rules = {\n",
    "    'GDP growth': (-50, 50),  # GDP growth should be within reasonable bounds\n",
    "    'population growth': (-10, 10),  # Population growth bounds\n",
    "    'inflation': (-50, 1000),  # Inflation bounds\n",
    "}\n",
    "\n",
    "validation_issues = 0\n",
    "for indicator_pattern, (min_val, max_val) in validation_rules.items():\n",
    "    matching_series = [series for series in df['Series Name'].unique() \n",
    "                      if pd.notna(series) and indicator_pattern.lower() in str(series).lower()]\n",
    "    \n",
    "    for series_name in matching_series:\n",
    "        mask = df['Series Name'] == series_name\n",
    "        for year_col in year_columns:\n",
    "            invalid_mask = (df.loc[mask, year_col] < min_val) | (df.loc[mask, year_col] > max_val)\n",
    "            invalid_count = invalid_mask.sum()\n",
    "            if invalid_count > 0:\n",
    "                validation_issues += invalid_count\n",
    "                # Set invalid values to NaN\n",
    "                df.loc[mask & invalid_mask, year_col] = pd.NA\n",
    "\n",
    "print(f\"Data validation issues found and corrected: {validation_issues}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: ENHANCED COUNTRY CLASSIFICATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüåç Step 6: Enhanced country classification...\")\n",
    "\n",
    "# Load World Bank income classifications\n",
    "url = \"https://databank.worldbank.org/data/download/site-content/CLASS.xlsx\"\n",
    "column_names = [\"Country Name\", \"Country Code\", \"Region\", \"Income Level\", \"Lending Category\", \"Extra\"]\n",
    "\n",
    "try:\n",
    "    income_df = pd.read_excel(url, sheet_name=\"List of economies\", skiprows=3, header=None, names=column_names)\n",
    "    income_df = income_df[[\"Country Name\", \"Country Code\", \"Income Level\"]]\n",
    "    print(\"‚úÖ Successfully loaded World Bank income classifications\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è Could not load online classifications, using manual mapping\")\n",
    "    # Fallback manual classification (you can expand this)\n",
    "    income_df = pd.DataFrame({\n",
    "        'Country Name': ['Afghanistan', 'Bangladesh', 'India', 'Brazil', 'China'],\n",
    "        'Country Code': ['AFG', 'BGD', 'IND', 'BRA', 'CHN'],\n",
    "        'Income Level': ['Low income', 'Lower middle income', 'Lower middle income', \n",
    "                        'Upper middle income', 'Upper middle income']\n",
    "    })\n",
    "\n",
    "# Merge with income classifications\n",
    "df = df.merge(income_df, on=[\"Country Name\", \"Country Code\"], how=\"left\")\n",
    "\n",
    "print(f\"Income level distribution before filtering:\")\n",
    "print(df[\"Income Level\"].value_counts())\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: ENHANCED SAMPLE FILTERING\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüîç Step 7: Enhanced sample filtering...\")\n",
    "\n",
    "# Remove regional aggregates and non-country entities (comprehensive list)\n",
    "regions_to_remove = [\n",
    "    \"World\", \"Sub-Saharan Africa\", \"OECD members\", \"East Asia & Pacific\", \"Europe & Central Asia\",\n",
    "    \"Middle East & North Africa\", \"Latin America & Caribbean\", \"South Asia\", \"North America\",\n",
    "    \"Fragile and conflict affected situations\", \"Least developed countries: UN classification\",\n",
    "    \"Low & middle income\", \"Lower middle income\", \"Middle income\", \"Upper middle income\",\n",
    "    \"Heavily indebted poor countries (HIPC)\", \"Small states\", \"IBRD only\", \"IDA & IBRD total\",\n",
    "    \"IDA blend\", \"IDA only\", \"IDA total\", \"Euro area\", \"European Union\",\n",
    "    \"East Asia & Pacific (IDA & IBRD countries)\", \"Europe & Central Asia (IDA & IBRD countries)\",\n",
    "    \"Latin America & the Caribbean (IDA & IBRD countries)\",\n",
    "    \"Middle East & North Africa (IDA & IBRD countries)\", \"Sub-Saharan Africa (IDA & IBRD countries)\",\n",
    "    \"Africa Eastern and Southern\", \"Africa Western and Central\", \"Arab World\", \"Caribbean small states\",\n",
    "    \"Central Europe and the Baltics\", \"Early-demographic dividend\", \"East Asia & Pacific (excluding high income)\",\n",
    "    \"Europe & Central Asia (excluding high income)\", \"High income\", \"Late-demographic dividend\",\n",
    "    \"Latin America & Caribbean (excluding high income)\", \"Low income\", \"Middle East & North Africa (excluding high income)\",\n",
    "    \"Other small states\", \"Pacific island small states\", \"Post-demographic dividend\",\n",
    "    \"Pre-demographic dividend\", \"South Asia (IDA & IBRD)\", \"Sub-Saharan Africa (excluding high income)\"\n",
    "]\n",
    "\n",
    "df = df[~df[\"Country Name\"].isin(regions_to_remove)]\n",
    "\n",
    "# Enhanced manual income level corrections with documentation\n",
    "manual_income_levels = {\n",
    "    \"Cote d'Ivoire\": \"Lower middle income\",\n",
    "    \"Sao Tome and Principe\": \"Lower middle income\", \n",
    "    \"Turkiye\": \"Upper middle income\",\n",
    "    \"Viet Nam\": \"Lower middle income\",\n",
    "    \"Afghanistan\": \"Low income\",\n",
    "    \"Venezuela, RB\": \"Not classified\",  # Economic crisis makes classification difficult\n",
    "    \"Aruba\": \"High income\",\n",
    "    \"Curacao\": \"High income\",\n",
    "    \"Czechia\": \"High income\"  # Updated classification\n",
    "}\n",
    "\n",
    "# Apply manual corrections\n",
    "df[\"Income Level\"] = df[\"Country Name\"].map(manual_income_levels).fillna(df[\"Income Level\"])\n",
    "\n",
    "# Remove high-income and unclassified countries (focus on development economics)\n",
    "df = df[~df[\"Income Level\"].isin([\"High income\", \"Not classified\"])]\n",
    "\n",
    "# Remove countries that recently graduated to high income (2024-2025 World Bank update)\n",
    "recently_graduated = [\"Bulgaria\", \"Palau\", \"Russian Federation\"]\n",
    "df = df[~df[\"Country Name\"].isin(recently_graduated)]\n",
    "\n",
    "print(f\"Final income level distribution:\")\n",
    "print(df[\"Income Level\"].value_counts())\n",
    "print(f\"Final country count: {df['Country Name'].nunique()}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: ENHANCED DATA AVAILABILITY ASSESSMENT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìä Step 8: Enhanced data availability assessment...\")\n",
    "\n",
    "# Assess data availability by country-series combination\n",
    "availability_stats = {}\n",
    "\n",
    "for income_level in df[\"Income Level\"].unique():\n",
    "    if pd.notna(income_level):\n",
    "        subset = df[df[\"Income Level\"] == income_level]\n",
    "        \n",
    "        # Calculate completion rate\n",
    "        total_possible = len(subset) * len(year_columns)\n",
    "        non_missing = subset[year_columns].count().sum()\n",
    "        completion_rate = (non_missing / total_possible) * 100\n",
    "        \n",
    "        availability_stats[income_level] = {\n",
    "            'countries': subset['Country Name'].nunique(),\n",
    "            'series': subset['Series Name'].nunique(),\n",
    "            'completion_rate': completion_rate\n",
    "        }\n",
    "        \n",
    "        print(f\"{income_level}: {subset['Country Name'].nunique()} countries, \"\n",
    "              f\"{subset['Series Name'].nunique()} series, \"\n",
    "              f\"{completion_rate:.1f}% data completion\")\n",
    "\n",
    "# Enhanced missing data threshold (series-specific)\n",
    "print(\"\\nApplying enhanced missing data thresholds...\")\n",
    "\n",
    "# Instead of arbitrary threshold, drop country-series with <30% data availability\n",
    "min_data_points = int(0.3 * len(year_columns))  # At least 30% of years must have data\n",
    "\n",
    "before_drop = len(df)\n",
    "df = df.dropna(thresh=len(non_year_columns) + min_data_points, axis=0)\n",
    "after_drop = len(df)\n",
    "\n",
    "print(f\"Dropped {before_drop - after_drop:,} observations with insufficient data\")\n",
    "print(f\"Retained {after_drop:,} observations ({(after_drop/before_drop)*100:.1f}%)\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 9: FINAL VALIDATION AND QUALITY CHECKS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n‚úÖ Step 9: Final validation and quality checks...\")\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated()\n",
    "if duplicates.sum() > 0:\n",
    "    print(f\"Warning: Found {duplicates.sum()} duplicate rows - removing them\")\n",
    "    df = df.drop_duplicates()\n",
    "else:\n",
    "    print(\"‚úÖ No duplicate rows found\")\n",
    "\n",
    "# Validate income classifications\n",
    "valid_income_levels = [\"Low income\", \"Lower middle income\", \"Upper middle income\"]\n",
    "invalid_income = df[~df[\"Income Level\"].isin(valid_income_levels)][\"Income Level\"].unique()\n",
    "if len(invalid_income) > 0:\n",
    "    print(f\"Warning: Invalid income levels found: {invalid_income}\")\n",
    "    df = df[df[\"Income Level\"].isin(valid_income_levels)]\n",
    "else:\n",
    "    print(\"‚úÖ All income classifications are valid\")\n",
    "\n",
    "# Final data quality summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìã FINAL DATA QUALITY SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"Final dataset shape: {df.shape}\")\n",
    "print(f\"Countries: {df['Country Name'].nunique()}\")\n",
    "print(f\"Series indicators: {df['Series Name'].nunique()}\")\n",
    "print(f\"Time span: {len(year_columns)} years\")\n",
    "\n",
    "# Missing data summary\n",
    "total_cells = len(df) * len(year_columns)\n",
    "missing_cells = df[year_columns].isnull().sum().sum()\n",
    "completion_rate = ((total_cells - missing_cells) / total_cells) * 100\n",
    "\n",
    "print(f\"Overall data completion rate: {completion_rate:.1f}%\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 10: SAVE ENHANCED DATASET\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüíæ Step 10: Saving enhanced dataset...\")\n",
    "\n",
    "# Reset index for clean saving\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save with detailed metadata\n",
    "output_path = '../datasets/enhanced_growth_rates_emissions_energy_prod_income_level_country_df.csv'\n",
    "df.to_csv(output_path, index=False)\n",
    "\n",
    "# Create metadata file\n",
    "metadata = {\n",
    "    'processing_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'total_observations': len(df),\n",
    "    'unique_countries': df['Country Name'].nunique(),\n",
    "    'unique_series': df['Series Name'].nunique(),\n",
    "    'time_span': f\"{year_columns[0]} to {year_columns[-1]}\",\n",
    "    'data_completion_rate': f\"{completion_rate:.1f}%\",\n",
    "    'income_distribution': df['Income Level'].value_counts().to_dict(),\n",
    "    'outliers_winsorized': total_outliers,\n",
    "    'validation_issues_corrected': validation_issues,\n",
    "    'processing_methods': [\n",
    "        'Within-group temporal imputation only',\n",
    "        'IQR-based outlier winsorization (5th-95th percentile)',\n",
    "        'Statistical validation rules applied',\n",
    "        'Enhanced country classification',\n",
    "        'Minimum 30% data availability threshold'\n",
    "    ]\n",
    "}\n",
    "\n",
    "metadata_df = pd.DataFrame([metadata])\n",
    "metadata_df.to_csv('../datasets/preprocessing_metadata.csv', index=False)\n",
    "\n",
    "print(f\"‚úÖ Enhanced dataset saved: {output_path}\")\n",
    "print(f\"‚úÖ Metadata saved: ../datasets/preprocessing_metadata.csv\")\n",
    "\n",
    "print(\"\\nüéâ ENHANCED PREPROCESSING COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(\"Key improvements over original preprocessing:\")\n",
    "print(\"‚Ä¢ Removed problematic cross-series imputation\")\n",
    "print(\"‚Ä¢ Added statistical outlier detection and winsorization\")\n",
    "print(\"‚Ä¢ Enhanced data quality validation\")\n",
    "print(\"‚Ä¢ Improved missing data thresholds\")\n",
    "print(\"‚Ä¢ Added comprehensive documentation and metadata\")\n",
    "print(\"‚Ä¢ Follows econometric best practices for panel data\")\n",
    "\n",
    "# ============================================================================\n",
    "# OPTIONAL: GENERATE PREPROCESSING REPORT\n",
    "# ============================================================================\n",
    "\n",
    "def generate_preprocessing_report():\n",
    "    \"\"\"Generate a comprehensive preprocessing report\"\"\"\n",
    "    \n",
    "    report = f\"\"\"\n",
    "# Enhanced Preprocessing Report\n",
    "Generated: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\n",
    "## Data Summary\n",
    "- **Total Observations**: {len(df):,}\n",
    "- **Countries**: {df['Country Name'].nunique()}\n",
    "- **Series Indicators**: {df['Series Name'].nunique()}\n",
    "- **Time Coverage**: {year_columns[0]} to {year_columns[-1]}\n",
    "- **Data Completion**: {completion_rate:.1f}%\n",
    "\n",
    "## Income Distribution\n",
    "{df['Income Level'].value_counts().to_string()}\n",
    "\n",
    "## Processing Steps Applied\n",
    "1. [DONE] Within-group temporal imputation\n",
    "2. [DONE] Statistical outlier detection (IQR method)\n",
    "3. [DONE] Data validation rules\n",
    "4. [DONE] Enhanced country classification\n",
    "5. [DONE] Minimum data availability thresholds\n",
    "6. [DONE] Comprehensive quality checks\n",
    "\n",
    "## Quality Improvements\n",
    "- **Outliers winsorized**: {total_outliers}\n",
    "- **Validation issues corrected**: {validation_issues}\n",
    "- **Completion rate**: {completion_rate:.1f}%\n",
    "- **No spurious cross-series imputation**\n",
    "\n",
    "## Ready for GMM Analysis\n",
    "This dataset follows econometric best practices and is suitable for:\n",
    "- Panel data analysis\n",
    "- Dynamic GMM estimation\n",
    "- IV/2SLS regression\n",
    "- Fixed effects models\n",
    "\"\"\"\n",
    "    \n",
    "    # Fix: Specify UTF-8 encoding explicitly\n",
    "    with open('../datasets/preprocessing_report.md', 'w', encoding='utf-8') as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    print(\"Report saved: ../datasets/preprocessing_report.md\")\n",
    "\n",
    "# Generate report\n",
    "generate_preprocessing_report()\n",
    "\n",
    "print(\"\\nüöÄ Dataset ready for GMM analysis!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
